# Data Pipeline for IT Logs Analysis

This mini-project demonstrates how to build a simple yet effective **data pipeline** for analyzing system log data using Python and pandas. The goal is to extract useful information from IT logs, such as identifying errors, tracking system behavior, and visualizing log activity over time.

---

##  Project Overview

In any IT environment, **logs** are generated to record important events like system startups, user logins, warnings, and errors. This project simulates such a log file and processes it using a custom-built pipeline.

### Pipeline Steps:
1. **Log Ingestion** â€“ Read raw logs from a `.txt` file.
2. **Parsing** â€“ Extract timestamps, log levels (INFO, WARNING, ERROR), and message content.
3. **Data Cleaning** â€“ Handle missing values and remove duplicates.
4. **Transformation** â€“ Extract additional time-based features like date, hour, and weekday.
5. **Analysis** â€“ Count log entries by hour and log level.
6. **Visualization** â€“ Plot log activity using `matplotlib` and `seaborn`.
7. **Export** â€“ Save the cleaned data to a `.csv` file for future use.

---

## ðŸ§ª Sample Log Format

```text
2025-05-14 08:22:11,INFO,System boot successful
2025-05-14 08:24:45,WARNING,Memory usage exceeds 70%
2025-05-14 08:26:12,ERROR,Database connection failed
